{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORKiCXH94Mm2cFN1XLv2tB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Gradient Checking**"],"metadata":{"id":"75kg4rj9Es5Z"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cU-hUqrEEscJ","executionInfo":{"status":"ok","timestamp":1759025444416,"user_tz":-330,"elapsed":17385,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}},"outputId":"108f2670-d1e0-48dd-e53a-7cb0c7d87980"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/Deep Learning/C2/Practical Aspects of Deep Learning/Gradient Checking')"],"metadata":{"id":"N1Xg8tnCFpnF","executionInfo":{"status":"ok","timestamp":1759025515097,"user_tz":-330,"elapsed":10,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip install dlai_tools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pTvyKJMF5UB","executionInfo":{"status":"ok","timestamp":1759025599985,"user_tz":-330,"elapsed":8233,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}},"outputId":"eb3b973a-33f9-402d-9b06-c29802036e50"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dlai_tools\n","  Downloading dlai_tools-0.6.1-py3-none-any.whl.metadata (2.5 kB)\n","Downloading dlai_tools-0.6.1-py3-none-any.whl (13 kB)\n","Installing collected packages: dlai_tools\n","Successfully installed dlai_tools-0.6.1\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"wH5W_MpjEloA","executionInfo":{"status":"ok","timestamp":1759025611917,"user_tz":-330,"elapsed":4807,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}}},"outputs":[],"source":["import numpy as np\n","from testCases import *\n","from public_tests import *\n","from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"]},{"cell_type":"markdown","source":["**Problem Statement**"],"metadata":{"id":"9XWw2QwGGnwL"}},{"cell_type":"markdown","source":["You are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud--whenever someone makes a payment, you want to see if the payment might be fraudulent, such as if the user's account has been taken over by a hacker.\n","\n","You already know that backpropagation is quite challenging to implement, and sometimes has bugs. Because this is a mission-critical application, your company's CEO wants to be really certain that your implementation of backpropagation is correct. Your CEO says, \"Give me proof that your backpropagation is actually working!\" To give this reassurance, you are going to use \"gradient checking.\""],"metadata":{"id":"q33K6Bj-GrD_"}},{"cell_type":"markdown","source":["**1-Dimensional Gradient Checking**"],"metadata":{"id":"hTuAdcbxG67j"}},{"cell_type":"markdown","source":["forward_propagation"],"metadata":{"id":"kgivoPiIG-gj"}},{"cell_type":"code","source":["# forward_propagation\n","\n","def forward_propagation(x, theta):\n","\n","    J = theta * x\n","\n","    return J\n"],"metadata":{"id":"gHZZf_SRGN2g","executionInfo":{"status":"ok","timestamp":1759025961222,"user_tz":-330,"elapsed":7,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["x, theta = 2, 4\n","J = forward_propagation(x, theta)\n","print (\"J = \" + str(J))\n","forward_propagation_test(forward_propagation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nMLEF36HjTC","executionInfo":{"status":"ok","timestamp":1759025970061,"user_tz":-330,"elapsed":47,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}},"outputId":"df5ca90d-5fb9-4b2a-feb6-e030fbd27137"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["J = 8\n","\u001b[92m All tests passed.\n"]}]},{"cell_type":"markdown","source":["backward_propagation"],"metadata":{"id":"YglfguuYHpye"}},{"cell_type":"code","source":["# backward_propagation\n","\n","def backward_propagation(x, theta):\n","\n","    dtheta = x\n","\n","    return dtheta\n"],"metadata":{"id":"XFaumXRHHpX1","executionInfo":{"status":"ok","timestamp":1759026033570,"user_tz":-330,"elapsed":51,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["x, theta = 3, 4\n","dtheta = backward_propagation(x, theta)\n","print (\"dtheta = \" + str(dtheta))\n","backward_propagation_test(backward_propagation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4D139o84H02F","executionInfo":{"status":"ok","timestamp":1759026042047,"user_tz":-330,"elapsed":14,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}},"outputId":"ddc131d8-411c-41f9-bee6-c15afadf4082"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["dtheta = 3\n","\u001b[92m All tests passed.\n"]}]},{"cell_type":"markdown","source":["gradient_check"],"metadata":{"id":"0nJOBUzRH_fK"}},{"cell_type":"code","source":["#  gradient_check\n","\n","def gradient_check(x, theta, epsilon=1e-7, print_msg=False):\n","\n","    # Compute gradapprox using right side of formula (1). epsilon is small enough, you don't need to worry about the limit.\n","\n","    theta_plus = theta + epsilon                   # Step 1\n","    theta_minus = theta - epsilon                  # Step 2\n","    J_plus = forward_propagation(x, theta_plus)    # Step 3\n","    J_minus = forward_propagation(x, theta_minus)  # Step 4\n","    gradapprox = (J_plus - J_minus) / (2 * epsilon)# Step 5\n","\n","    # Check if gradapprox is close enough to the output of backward_propagation()\n","\n","    grad = backward_propagation(x, theta)\n","\n","\n","    # Compute difference\n","\n","    numerator = np.linalg.norm(grad - gradapprox)      # Step 1'\n","    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)  # Step 2'\n","    difference = numerator / denominator               # Step 3'\n","\n","\n","    if print_msg:\n","        if difference > 2e-7:\n","            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n","        else:\n","            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n","\n","    return difference\n"],"metadata":{"id":"AT5WsHzMH1tw","executionInfo":{"status":"ok","timestamp":1759026169817,"user_tz":-330,"elapsed":14,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["x, theta = 3, 4\n","difference = gradient_check(x, theta, print_msg=True)\n","gradient_check_test(gradient_check, difference)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vyq4sGWQIWAz","executionInfo":{"status":"ok","timestamp":1759026178420,"user_tz":-330,"elapsed":45,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}},"outputId":"f6c11680-4916-4ea0-d389-c9082246da2c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92mYour backward propagation works perfectly fine! difference = 7.814075313343006e-11\u001b[0m\n","\u001b[92m All tests passed.\n"]}]},{"cell_type":"markdown","source":["**N-Dimensional Gradient Checking**"],"metadata":{"id":"Oi5NBWb3IaUa"}},{"cell_type":"markdown","source":["forward_propagation_n"],"metadata":{"id":"t2bTZ9-II1il"}},{"cell_type":"code","source":["def forward_propagation_n(X, Y, parameters):\n","\n","    # retrieve parameters\n","    m = X.shape[1]\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","\n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = relu(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = relu(Z2)\n","    Z3 = np.dot(W3, A2) + b3\n","    A3 = sigmoid(Z3)\n","\n","    # Cost\n","    log_probs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n","    cost = 1. / m * np.sum(log_probs)\n","\n","    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n","\n","    return cost, cache"],"metadata":{"id":"7LIQbQUrIaAi","executionInfo":{"status":"ok","timestamp":1759026283708,"user_tz":-330,"elapsed":13,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["backward_propagation_n"],"metadata":{"id":"uzatMl1bI_QI"}},{"cell_type":"code","source":["def backward_propagation_n(X, Y, cache):\n","\n","    m = X.shape[1]\n","    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","\n","    dZ3 = A3 - Y\n","    dW3 = 1. / m * np.dot(dZ3, A2.T)\n","    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n","\n","    dA2 = np.dot(W3.T, dZ3)\n","    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2\n","    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n","\n","    dA1 = np.dot(W2.T, dZ2)\n","    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","    dW1 = 1. / m * np.dot(dZ1, X.T)\n","    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True)\n","\n","    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n","                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n","                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","\n","    return gradients"],"metadata":{"id":"4_buJIWOIw00","executionInfo":{"status":"ok","timestamp":1759026339261,"user_tz":-330,"elapsed":41,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["gradient_check_n"],"metadata":{"id":"7rLyiMSwJSAG"}},{"cell_type":"code","source":["#  gradient_check_n\n","\n","def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n","\n","    # Convert parameters and gradients to vectors\n","    parameters_values, _ = dictionary_to_vector(parameters)\n","    grad = gradients_to_vector(gradients)\n","\n","    num_parameters = parameters_values.shape[0]\n","    J_plus = np.zeros((num_parameters, 1))\n","    J_minus = np.zeros((num_parameters, 1))\n","    gradapprox = np.zeros((num_parameters, 1))\n","\n","    # Compute gradapprox\n","    for i in range(num_parameters):\n","        # Compute J_plus[i]\n","        theta_plus = np.copy(parameters_values)\n","        theta_plus[i][0] += epsilon\n","        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))\n","\n","        # Compute J_minus[i]\n","        theta_minus = np.copy(parameters_values)\n","        theta_minus[i][0] -= epsilon\n","        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))\n","\n","        # Compute gradapprox[i]\n","        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n","\n","    # Compute difference\n","    numerator = np.linalg.norm(grad - gradapprox)\n","    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n","    difference = numerator / denominator\n","\n","    if print_msg:\n","        if difference > 2e-7:\n","            print(\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n","        else:\n","            print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n","\n","    return difference\n"],"metadata":{"id":"qDGPx3Z-JMRB","executionInfo":{"status":"ok","timestamp":1759026509307,"user_tz":-330,"elapsed":31,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["X, Y, parameters = gradient_check_n_test_case()\n","\n","cost, cache = forward_propagation_n(X, Y, parameters)\n","gradients = backward_propagation_n(X, Y, cache)\n","difference = gradient_check_n(parameters, gradients, X, Y, 1e-7, True)\n","expected_values = [0.2850931567761623, 1.1890913024229996e-07]\n","assert not(type(difference) == np.ndarray), \"You are not using np.linalg.norm for numerator or denominator\"\n","assert np.any(np.isclose(difference, expected_values)), \"Wrong value. It is not one of the expected values\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ba4djKLTJpOz","executionInfo":{"status":"ok","timestamp":1759026518886,"user_tz":-330,"elapsed":43,"user":{"displayName":"Rahul Das","userId":"10605675281746257009"}},"outputId":"462f1fbf-5535-4126-a7a2-b636f4571fb0"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[93mThere is a mistake in the backward propagation! difference = 0.2850931567761624\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"Pw8RxHiXEV3X"},"source":["\n","**Notes**\n","- Gradient Checking is slow! Approximating the gradient with $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ is computationally costly. For this reason, we don't run gradient checking at every iteration during training. Just a few times to check if the gradient is correct.\n","- Gradient Checking, at least as we've presented it, doesn't work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout.\n","\n","Congrats! Now you can be confident that your deep learning model for fraud detection is working correctly! You can even use this to convince your CEO. :)\n","<br>\n","<font color='blue'>\n","    \n"]},{"cell_type":"code","source":[],"metadata":{"id":"YnJBYiGJKPfk"},"execution_count":null,"outputs":[]}]}